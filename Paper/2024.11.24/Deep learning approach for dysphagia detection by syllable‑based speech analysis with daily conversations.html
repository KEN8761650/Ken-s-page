<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>2024.11.21 Paper Japanese</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 10px;
            padding: 0;
        }
        h1 {
            font-size: 25px;
            width: 130px;
            background-color: aqua;
            border: 2px solid red;
            padding: 5px;
        }
        h2 {
            font-size: 30px;
            width: 250px;
            background-color: rgb(21, 184, 86);
            border: 2px solid red;
            padding: 5px;

            margin: 30px 0; /* 為了便於測試，在標題上設置較大的上下外邊距 */
            scroll-margin-top: 50vh; /* 確保標題在螢幕中間位置 */
        }
        li {
            font-size: 15px;
        }
        .sticky-container {
            position: sticky;
            top: 0; /* 距離頁面頂端 0px */
            background-color: #f0f0f0; /* 背景色讓容器更醒目 */
            padding: 5px;
            z-index: 10; /* 確保容器在其他元素之上 */
        }
        p {
            font-size: 20px;        
        }
        div>#theme{
            color: brown;
            background-color: bisque;
            font-size: 25px;
        }
        div>#keyword, div>#association {
            display: block;
            padding: 3px;
            float: right;
            clear: right;
            /* 確保每個元素在右側獨立排列 請確保 HTML 結構中 #association 是緊隨 #keyword 之後*/
        }
        div>#keyword {
            color: red;
        }
        div>#association {
            color: blue;
        }
    </style>
</head>
<body>
    <div class="sticky-container">
        <span id="theme">日常会話における音節ベースの音声分析を用いた嚥下障害検出のためのディープラーニングアプローチ</span><br>
        <span id="keyword">キーワード:嚥下障害、ディープラーニング、会話、音節ベースの音声分析、音声認識モデル、人工知能</span>
        <span id="association">Scientific Reports (Sci Rep) ISSN 2045-2322 (online) Sci Rep 14, 20270 (2024)</span>
        <h1>Index</h1>
        <ul>
            <li><a href="#Purpose">Purpose</a></li>
            <li><a href="#Patients">Patients</a></li>
            <li><a href="#Method">Method</a></li>
            <li><a href="#Results">Results</a></li>
            <li><a href="#Conclusion">Conclusion</a></li>
            <li><a href="#Limitations">Limitations</a></li>
        </ul>
    </div>
    <br><br><br><br><br>
    <h2 id="Purpose">Purpose</h2>
        <p>目的：<br>
            1.開発したディープラーニングモデル:<br>
            日常会話の音声データを用いて嚥下障害を検出するディープラーニングモデルを開発しました。このモデルでは、畳み込みニューラルネットワーク（CNN）を用いて音声データを解析します。<br>           
            2.音節ベースの音声分析の新規性:<br>
            これまでの研究では、音節ベースの音声分析を嚥下障害の検出に使用した例はありません。本研究では、この新しいアプローチに基づいてディープラーニングモデルを構築しました。このモデルは、音声認識（STT: Speech-to-Text）モデルを使用して日常会話を音節に分割し、音節ベースの音声分析を通じて効果的に嚥下障害を識別することを目的としています。</p>
    <br><br><br><br><br>
    <h2 id="Method">Method</h2>
        <p>方法：<br>
            提案されたモデルの有効性はデータ全体の音節分割分析と個別レベルの2つのアプローチで検証されました。<br>
            また、会話の音声データは、STT（音声認識）モデルを用いて音節に分割されました。この音節分割された音声データは、ログメルスペクトログラムにより2次元形式に変換されました。変換後のデータは、CNNベースのモデルを用いて訓練およびテストに利用されました。本研究の全体的なプロセスは、 <a href="./Fig1.png" target="_blank">図1</a>に示されています。<br>
            1.対象者：2023年7月から11月の期間に、単一の三次医療大学病院で嚥下障害が疑われた患者です。基準は、年齢19歳以上、嚥下障害が疑われ、嚥下造影検査（VFSS）を受けたこと、同意書に署名したこと、参加者の大多数は脳卒中患者でした。<br>
            2.データ収集：全ての会話データは、患者と単一の研究者との間で行われたもので、背景ノイズを排除するために個別の空間で録音されました。録音には、Galaxy S22（Samsung、韓国水原市）内蔵のマイクを使用しました。マイクは患者の顔から30cm以内の距離に設置されました。会話の内容に固定されたテーマはなく、各参加者ごとに約10分間録音が行われました。<br>
            3.データソース：録音された音声データはWAV形式で保存され、品質は44.1 kHz、16ビットでした。Audacityソフトウェアを用いて、それぞれの患者の音声のみが含まれるよう音声を分離しました。参加者のデータはVFSS（嚥下造影検査）の結果に基づいて嚥下障害群またはコントロール群に分類されました。嚥下障害ダイエットには液体や半固体を含む4種類の異なる粘度の食品が用いられ、これらを造影剤と混ぜてテストしました。各種類の食品に対し、小量（3 cc）と大量（7 cc）の2つの容量が使用されました。<br>
            4.データ構造：音声データは2つのグループに分けられました：1.コントロール群: 24人（16人の参加者とYouTubeから取得した8人分のデータ）2.嚥下障害群: 16人<br>
            データ全体の長さは、コントロール群が5109秒、嚥下障害群が3461秒でした。この音声データを音節単位に分割した結果、コントロール群は15,980音節、嚥下障害群は7830音節に分割されました。分割されたデータはトレーニングセットとテストセットに分けられ、トレーニング用に18,951音節、テスト用に4859音節が割り当てられました。本研究では、トレーニングデータを最大化するため、検証セットは除外されました。データ構造の詳細は<a href="./Fig2.png" target="_blank">図2</a>に示されています。<br>
            5.データ生成：会話音声データの音節分割にはSTTモデルであるClova Speech20が使用されました。データ生成プロセスの詳細は<a href="./Fig3.png" target="_blank">図3</a>に示されています。トレーニングデータセットの数と多様性を増加させるため、以下の手法が採用されました：<br>
            &nbsp;&nbsp;a.開始点を0～0.2秒のランダムな範囲内で選択し、0.6秒のセグメントを生成<br>
            &nbsp;&nbsp;b.生成したセグメントにノイズ音を追加<br>
            最終的に、正常群の12,618音節と嚥下障害群の6333音節から、各グループごとに10,000件の音声データが拡張されました。<br>
            6.検出モデル：音節レベルの音声データの学習とテストには、畳み込みニューラルネットワーク（CNN）ベースのモデルを使用しました。具体的には、ResNet-34モデルを採用しました。このモデルは、学習が容易で、優れた性能を発揮し、精度を向上させる特徴を備えているほか、複雑な特徴を学習する能力があります。<br>
            &nbsp;&nbsp;学習設定：エポック数: 5、ミニバッチサイズ: 16、学習率: 0.001、損失関数: クロスエントロピー損失、最適化手法: Adamオプティマイザー、ResNet-34の入力データとアーキテクチャの詳細は<a href="./Fig4.png" target="_blank">図4</a>に示されています。<br>
            7.開発環境：<br>
            &nbsp;&nbsp;&nbsp;&nbsp;ソフトウェア: Python 3.11.4, PyTorch 2.0.1<br>
            &nbsp;&nbsp;&nbsp;&nbsp;CPU: 第13世代 Intel® Core™ i9-13900K プロセッサ<br>
            &nbsp;&nbsp;&nbsp;&nbsp;GPU: GeForce RTX 4090 (24GB) GPU<br>
            &nbsp;&nbsp;&nbsp;&nbsp;メモリ: 64GB RAM<br>
            &nbsp;&nbsp;&nbsp;&nbsp;システム: Windows 11 Pro<br>
            &nbsp;&nbsp;&nbsp;&nbsp;STTモデル: Clova Speech API バージョン1.9.0<br>
            &nbsp;&nbsp;&nbsp;&nbsp;ログメルスペクトログラム変換: Librosa Pythonライブラリ（高度な信号処理を簡易に実装可能なオーディオ解析関数を提供）<br>
            8.統計解析: <br>
            2つの評価手法を適用しました： <br>
            バイナリ分類:各音節データを「嚥下障害」または「正常」と分類。<br>
            個人ベースの評価:テストセットを個人単位で分析し、個人の会話中に嚥下障害が存在するかを評価。各個人のデータセット内で「嚥下障害」と分類された音節データの割合に基づき、個人の嚥下障害の有無を決定しました。<br>
            音節データの50%以上が「嚥下障害」と分類された場合、その個人は嚥下障害と判断。50%未満の場合、正常と分類。<br>
            この2つの評価方法の詳細は<a href="./FIg5.png" target="_blank">図5</a>に示されています。
            </p>
    <br><br><br><br><br>
    <h2 id="Results">Results</h2>
        <p>結果：<br>
            1.参加者の人口統計学的特徴およびYouTubeデータ情報：参加者の人口統計学的特徴およびYouTubeデータに関する情報は<a href="./Table1.png" target="_blank">表1</a>に示されています。<br>
            2.提案モデルの性能：(本研究で提案したモデルは、嚥下障害の診断において以下の性能を示しました)<br>
            &nbsp;&nbsp;&nbsp;&nbsp;総合的な精度: 0.794<br>
            &nbsp;&nbsp;&nbsp;&nbsp;感度: 0.901<br>
            &nbsp;&nbsp;&nbsp;&nbsp;特異度: 0.687<br>
            &nbsp;&nbsp;&nbsp;&nbsp;陽性的中率（PPV）: 0.742<br>
            &nbsp;&nbsp;&nbsp;&nbsp;陰性的中率（NPV）: 0.874<br>
            これらの結果は<a href="./Fig6.png" target="_blank">図6</a>に示されています。<br>
            3.個人レベル評価：(40人全員を対象とした個人レベル評価の結果)<br>            
            &nbsp;&nbsp;&nbsp;&nbsp;コントロール群の精度: 0.833<br>
            &nbsp;&nbsp;&nbsp;&nbsp;嚥下障害群の精度: 1.000<br>
            &nbsp;&nbsp;&nbsp;&nbsp;総合的な精度: 0.900<br>
            さらに、ROC曲線下面積（AUC）は0.953を示しました <a href="./Fig7.png" target="_blank">（図7参照）。</a></p>
    <br><br><br><br><br>
    <h2 id="Conclusion">Conclusion</h2>
        <p>結論：<br>
            本研究は、嚥下障害の診断に向けた新しい方法論を提案しました。日常会話の音声データをSTTモデルで音節に分割し、新たに開発したディープラーニングモデルを用いて、このデータを解析し、嚥下障害と正常状態を区別しました。<br>
            日常会話の音声データを使用することで、患者の日常生活環境に近い状況を再現できる可能性が示唆されました。提案モデルは嚥下障害のある被験者を正確に診断し、これまでの研究と比較して遜色のない成果を示しました。<br>
            しかし、嚥下障害のない正常な被験者4名が嚥下障害と誤分類され、そのうち3名は構音障害（dysarthria）を患っていました。この結果を踏まえ、将来的な研究では、会話音声を用いて構音障害と嚥下障害を区別する方法の開発が求められる可能性があります。</p>
    <br><br><br><br><br>
    <h2 id="Limitations">Limitations</h2>
        <p>本研究の限界：<br>
            1.サンプルサイズの小規模性：本研究は比較的小規模なサンプルサイズで実施されました。より大規模なサンプルサイズを用いた追加研究により、結果の一般化可能性と頑健性が向上すると期待されます。<br>
            2.嚥下障害の重症度を考慮していない点：本研究では、ディープラーニングを用いた解析において、嚥下障害の重症度は考慮されていませんでした。重症度を反映した分析が、今後の研究課題として重要です。<br>
            3.YouTubeから得られたデータの品質：YouTubeから取得したデータは、参加者から収集した録音データと同等の品質である保証がありません。この点がモデルの性能に影響を与えた可能性があります。<br>
            4.検証データセットの欠如：トレーニングセットのサイズを最大化するため、検証データセットを含めていません。この選択はモデルの評価に制約をもたらした可能性があります。しかし、独立した検証データセットを利用することで、過学習のリスクを軽減し、モデルの性能をより頑健に評価できた可能性があります。</p>
</body>
</html>